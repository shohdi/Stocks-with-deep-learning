[defaults]
cuda=True
reload_config=False
env=Breakout-v0
; how many steps to use in reward estimation
n_steps=10
; initial epsilon
epsilon=1.0
; epsilon decay after every epoch
epsilon_decay=0.995
; minimum value of epsilon, if not specified, no minimum
epsilon_minimum=0.15
; save interval in epoch
save_epoches=50
; count of games in the pool
env_pool_size=10
; convert image to grayscale, default=True
grayscale=True
; image width, default=80
image_width=80
; image height, default=80
image_height=80
; framebuffer depth
frames_count=4

[dqn]
copy_target_net_every_epoch=5
; use frozen network for target Q values
target_dqn=True
; chose target (last) action by basic model, but Q from frozen
double_dqn=False
; dueling net architecture
dueling=False

[stop]
; stop after this mean reward for given amount of games, this is an optional param
mean_games=100
mean_reward=1000.0

[learning]
lr=0.0005
; how to decay LR every epoch, if not specified -- disabled
lr_decay=1.0
; minimum value of LR
lr_minimum=0.0002
batch_size=512

[exp_buffer]
; how many episodes we'll keep in buffer
size=20000
; how many entries we'll fetch every epoch
populate=2000
; how many batches we'll sample every epoch
epoch_batches = 10
prio_alpha = 0.0
prio_beta = 0.5

